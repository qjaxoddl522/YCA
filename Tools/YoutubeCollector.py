# ===========================================
# YouTube Data API ê¸°ë°˜ ì˜ìƒ ë° ëŒ“ê¸€ ìˆ˜ì§‘ (Unity ì—°ë™ìš© ìë™ ì‹¤í–‰ ë²„ì „)
# - argparseë¡œ ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬ (Unityì—ì„œ ì‹¤í–‰ ê°€ëŠ¥)
# - í‚¤ì›Œë“œ ê²€ìƒ‰ ë˜ëŠ” ë§í¬ ì§ì ‘ ì…ë ¥
# - ë‚ ì§œ ì œí•œ í•„ìˆ˜ ì„¤ì •
# - ì½˜ì†” ì¶œë ¥: ì˜ìƒ ê°œìˆ˜ + ì œëª©ë§Œ í‘œì‹œ
# - ê²°ê³¼ CSV ìë™ ì €ì¥
# ===========================================

# !pip install google-api-python-client pandas tqdm

import argparse
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import pandas as pd
import datetime
import time
import re
import sys

# ===========================================
# [1] API ì—°ê²° ì„¤ì •
# ===========================================
API_KEY = "AIzaSyDiucMkEW5MfQbrExLa5CDop_34c0l98TU"  # ğŸ”‘ ë³¸ì¸ YouTube Data API í‚¤ ì…ë ¥
youtube = build("youtube", "v3", developerKey=API_KEY)

# ===========================================
# [2] ìœ íŠœë¸Œ ì˜ìƒ ID ì¶”ì¶œ
# ===========================================
def extract_video_id(url):
    """ìœ íŠœë¸Œ ë§í¬ì—ì„œ video_id ì¶”ì¶œ"""
    pattern = r"(?:v=|youtu\.be/)([a-zA-Z0-9_-]{11})"
    match = re.search(pattern, url)
    return match.group(1) if match else None

# ===========================================
# [3] ëŒ“ê¸€ ìˆ˜ì§‘ (ì¸ê¸°ìˆœ, ìµœëŒ€ 100ê°œ)
# ===========================================
def get_video_comments(video_id, max_results=100):
    """íŠ¹ì • ì˜ìƒì˜ ìƒìœ„ ëŒ“ê¸€ ìˆ˜ì§‘"""
    comments = []
    next_page_token = None
    collected = 0
    retries = 3

    for attempt in range(retries):
        try:
            while collected < max_results:
                response = youtube.commentThreads().list(
                    part="snippet",
                    videoId=video_id,
                    maxResults=min(100, max_results - collected),
                    order="relevance",  # ì¸ê¸°ìˆœ
                    textFormat="plainText",
                    pageToken=next_page_token
                ).execute()

                for item in response.get("items", []):
                    snippet = item["snippet"]["topLevelComment"]["snippet"]
                    comments.append({
                        "author": snippet["authorDisplayName"],
                        "text": snippet["textDisplay"],
                        "like_count": snippet["likeCount"],
                        "published": snippet["publishedAt"]
                    })

                collected += len(response.get("items", []))
                next_page_token = response.get("nextPageToken")
                if not next_page_token:
                    break
            break
        except HttpError as e:
            print(f"ëŒ“ê¸€ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨ ({attempt+1}/{retries}) ì¬ì‹œë„ ì¤‘...")
            time.sleep(2)
    return comments

# ===========================================
# [4] ì˜ìƒ ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘
# ===========================================
def get_video_info(video_id):
    """ì˜ìƒì˜ ê¸°ë³¸ ì •ë³´ + ëŒ“ê¸€ ìˆ˜ì§‘"""
    retries = 3
    for attempt in range(retries):
        try:
            video_response = youtube.videos().list(
                part="snippet,statistics",
                id=video_id
            ).execute()

            if not video_response["items"]:
                return None

            item = video_response["items"][0]
            title = item["snippet"]["title"]
            thumbnail = item["snippet"]["thumbnails"]["high"]["url"]
            views = item["statistics"].get("viewCount", "0")
            likes = item["statistics"].get("likeCount", "0")
            published_at = item["snippet"]["publishedAt"]

            comments = get_video_comments(video_id, max_results=100)

            return {
                "video_id": video_id,
                "title": title,
                "thumbnail": thumbnail,
                "views": views,
                "likes": likes,
                "published_at": published_at,
                "link": f"https://www.youtube.com/watch?v={video_id}",
                "comments": comments
            }
        except HttpError:
            print(f"ì˜ìƒ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨ ({attempt+1}/{retries}) ì¬ì‹œë„ ì¤‘...")
            time.sleep(2)
    return None

# ===========================================
# [5] ë‚ ì§œ ì œí•œ ê³„ì‚° í•¨ìˆ˜
# ===========================================
def get_published_after(period_type, amount):
    """ë…„/ì›”/ì£¼ ë‹¨ìœ„ë¡œ ë‚ ì§œ ì œí•œ ê³„ì‚°"""
    now = datetime.datetime.utcnow()

    if period_type == "year":
        delta = datetime.timedelta(days=amount * 365)
    elif period_type == "month":
        delta = datetime.timedelta(days=amount * 30)
    elif period_type == "week":
        delta = datetime.timedelta(weeks=amount)
    else:
        print("âŒ ì˜ëª»ëœ ê¸°ê°„ ë‹¨ìœ„ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’(3ë…„)ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        delta = datetime.timedelta(days=3 * 365)

    return (now - delta).isoformat("T") + "Z"

# ===========================================
# [6] í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰
# ===========================================
def search_videos_with_comments(keyword, published_after=None, max_results=50):
    """í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰ í›„ ëŒ“ê¸€ í¬í•¨ ìˆ˜ì§‘"""
    print(f"\nğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰ ì¤‘...\n")

    search_response = youtube.search().list(
        q=keyword,
        part="snippet",
        type="video",
        maxResults=max_results,
        publishedAfter=published_after
    ).execute()

    results = []
    items = search_response["items"]
    
    for idx, item in enumerate(items, 1):
        if item["id"]["kind"] == "youtube#video":
            video_id = item["id"]["videoId"]
            info = get_video_info(video_id)
            if info:
                results.append(info)
    return results

# ===========================================
# [7] ê²°ê³¼ ì¶œë ¥ ë° CSV ì €ì¥
# ===========================================
def display_and_save_results(results, filename="youtube_results.csv"):
    """ì˜ìƒ ì œëª©ë§Œ ì¶œë ¥ + ì „ì²´ ë°ì´í„° CSV ì €ì¥"""
    data_rows = []
    comment_count = 0

    print("\n==============================")
    print(f"ğŸ“º ìˆ˜ì§‘ëœ ì˜ìƒ ì œëª© ëª©ë¡ (ì´ {len(results)}ê°œ):\n")

    for idx, r in enumerate(results, start=1):
        print(f"{idx}. {r['title']}")
        for c in r["comments"]:
            comment_count += 1
            data_rows.append({
                "video_title": r["title"],
                "video_link": r["link"],
                "views": r["views"],
                "likes": r["likes"],
                "published_at": r["published_at"],
                "thumbnail": r["thumbnail"],
                "comment_author": c["author"],
                "comment_text": c["text"],
                "comment_likes": c["like_count"],
                "comment_published": c["published"]
            })

    df = pd.DataFrame(data_rows)
    df.to_csv(filename, index=False, encoding="utf-8-sig")

    print("\n==============================")
    print(f"âœ… ì´ ì˜ìƒ ìˆ˜ì§‘ ê°œìˆ˜: {len(results)}ê°œ")
    print(f"âœ… ì´ ëŒ“ê¸€ ìˆ˜ì§‘ ê°œìˆ˜: {comment_count}ê°œ")
    print(f"ğŸ’¾ '{filename}' íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ!")
    print("==============================\n")

# ===========================================
# [8] main ì‹¤í–‰ (Interactive Colab Version)
# - Unity ì—°ë™ ì‹œì—ëŠ” ì´ ë¶€ë¶„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ê³  ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
# ===========================================
def main():
    """Unityì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="YouTube ì˜ìƒ ë° ëŒ“ê¸€ ìˆ˜ì§‘ê¸° (Unity ì—°ë™ìš©)")
    parser.add_argument("--mode", choices=["keyword", "link"], required=True, help="ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ (keyword / link)")
    parser.add_argument("--text", type=str, help="ê²€ìƒ‰ í‚¤ì›Œë“œ (mode=keywordì¼ ë•Œ)")
    parser.add_argument("--url", type=str, help="ìœ íŠœë¸Œ ì˜ìƒ ë§í¬ (mode=linkì¼ ë•Œ)")
    parser.add_argument("--period_type", choices=["year", "month", "week"], required=True, help="ê¸°ê°„ ë‹¨ìœ„ ì„ íƒ")
    parser.add_argument("--amount", type=int, required=True, help="ê¸°ê°„ ìˆ˜ (ì˜ˆ: 3 â†’ 3ê°œì›”/3ë…„/3ì£¼)")
    parser.add_argument("--output", type=str, default="youtube_results.csv", help="ê²°ê³¼ CSV ì €ì¥ ê²½ë¡œ")

    # ì‹¤ì œ ëª…ë ¹ì¤„ ì¸ì íŒŒì‹± (sys.argv ì‚¬ìš©)
    args = parser.parse_args()

    published_after = get_published_after(args.period_type, args.amount)

    if args.mode == "keyword":
        if not args.text:
            print("âŒ --text ì¸ìê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        results = search_videos_with_comments(args.text, published_after=published_after)
        display_and_save_results(results, args.output)

    elif args.mode == "link":
        if not args.url:
            print("âŒ --url ì¸ìê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        video_id = extract_video_id(args.url)
        if not video_id:
            print("âŒ ì˜ìƒ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        info = get_video_info(video_id)
        if info:
            display_and_save_results([info], args.output)
        else:
            print("âŒ ì˜ìƒ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# ===========================================
# ì‹¤í–‰ íŒŒíŠ¸ (Colab í™˜ê²½ì—ì„œëŠ” main_colab() ì‹¤í–‰)
# - Unity ì—°ë™ ì‹œì—ëŠ” ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ì—¬ main() ì‹¤í–‰
# ===========================================
if __name__ == "__main__":
    main()