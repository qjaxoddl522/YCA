# ===============================================
# ðŸ“˜ YouTube CSV ìžë™ ë¶„ì„ê¸° (NER + ê°ì •ë¶„ì„ í†µí•©)
# - KoBERT+CRF ê°œì²´ëª… ì¸ì‹ (ì„ íƒ)
# - KoBERT/KcELECTRA ê°ì • ë¶„ì„
# - ëŒ“ê¸€/ì œëª© CSV ìžë™ íŒë³„ ë° ë¶„ì„
# ===============================================

import os, re, argparse
import pandas as pd
import numpy as np
import torch
from torch import nn
from datetime import datetime
from collections import Counter
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification

tqdm.pandas()
device = "cuda" if torch.cuda.is_available() else "cpu"

# ------------------------------
# ðŸ§© Args ì„¤ì •
# ------------------------------
class Args:
    def __init__(self, output=None, use_ner=False, sentiment_model=None,
                 neutral_threshold=0.60, min_len=3, topn=3):
        self.output = output
        self.use_ner = use_ner
        self.sentiment_model = sentiment_model
        self.neutral_threshold = neutral_threshold
        self.min_len = min_len
        self.topn = topn

# ------------------------------
# ðŸ§  ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ
# ------------------------------
def load_sentiment_model(model_choice=None):
    if model_choice == "kobert":
        name = "nlpai-lab/kobert-base-sentiment"
    elif model_choice == "kcelectra":
        name = "beomi/KcELECTRA-base-v2022"
    else:
        name = "nlpai-lab/kobert-base-sentiment"

    try:
        tok = AutoTokenizer.from_pretrained(name)
        model = AutoModelForSequenceClassification.from_pretrained(name).to(device).eval()
        label_map = {0: "ë¶€ì •", 1: "ì¤‘ë¦½", 2: "ê¸ì •"} if model.config.num_labels == 3 else {0: "ë¶€ì •", 1: "ê¸ì •"}
        return tok, model, label_map
    except Exception:
        tok = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base-v2022")
        model = AutoModelForSequenceClassification.from_pretrained("beomi/KcELECTRA-base-v2022").to(device).eval()
        label_map = {0: "ë¶€ì •", 1: "ê¸ì •"}
        return tok, model, label_map

# ------------------------------
# ðŸ§© NER (BERT+CRF)
# ------------------------------
def build_ner_extractor(use_ner=True):
    if not use_ner or not os.path.exists("kobert_crf_ner.pt"):
        print("NER ëª¨ë¸ ë¹„í™œì„±í™” â€” ì •ê·œì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì‚¬ìš©")
        return extract_keywords_regex

    try:
        from torchcrf import CRF
    except ImportError:
        print("torchcrf ë¯¸ì„¤ì¹˜ â€” NER ë¹„í™œì„±í™”, ì •ê·œì‹ ì‚¬ìš©")
        return extract_keywords_regex

    class BertCRF(nn.Module):
        def __init__(self, bert, num_labels):
            super().__init__()
            self.bert = bert
            self.dropout = nn.Dropout(0.1)
            self.fc = nn.Linear(bert.config.hidden_size, num_labels)
            self.crf = CRF(num_labels)

        def decode_tags(self, input_ids, attention_mask):
            with torch.no_grad():
                outputs = self.bert(input_ids, attention_mask=attention_mask)[0]
                emissions = self.fc(self.dropout(outputs)).transpose(0, 1)
                mask_t = attention_mask.transpose(0, 1).bool()
                preds = self.crf.decode(emissions, mask=mask_t)
                return preds

    id2label = {0: "O", 1: "B-ORG", 2: "I-ORG", 3: "B-LOC", 4: "I-LOC"}
    num_labels = len(id2label)
    tokenizer_ner = AutoTokenizer.from_pretrained("skt/kobert-base-v1")
    bert = AutoModel.from_pretrained("skt/kobert-base-v1")

    model_ner = BertCRF(bert, num_labels).to(device)
    model_ner.load_state_dict(torch.load("kobert_crf_ner.pt", map_location=device))
    model_ner.eval()
    print("KoBERT+CRF NER ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def extract_keywords_ner(text):
        tokens = re.findall(r"[ê°€-íž£a-zA-Z0-9]{2,}", str(text))
        if not tokens:
            return ""
        max_tokens = 256
        if len(tokens) > max_tokens:
            tokens = tokens[:max_tokens]
        enc = tokenizer_ner(
            tokens,
            is_split_into_words=True,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=512,
        )
        input_ids = enc["input_ids"].to(device)
        mask = enc["attention_mask"].to(device)
        preds = model_ner.decode_tags(input_ids, mask)
        if isinstance(preds, torch.Tensor):
            preds = preds.tolist()
        if isinstance(preds[0], torch.Tensor):
            preds = [p.tolist() for p in preds]
        if isinstance(preds[0], int):
            tags = preds[:len(tokens)]
        else:
            tags = preds[0][:len(tokens)]
        ents = [tok for tok, tid in zip(tokens, tags) if id2label[tid] != "O"]
        return " | ".join(ents[:3])

    return extract_keywords_ner

# ------------------------------
# ðŸ§¹ ëŒ“ê¸€ í•„í„° & í‚¤ì›Œë“œ ì¶”ì¶œ
# ------------------------------
def is_spam_or_timeline(text, min_len=3):
    if not isinstance(text, str):
        return True
    text = text.strip().lower()
    if len(text) < min_len:
        return True
    if re.search(r"\b\d{1,2}:\d{2}(?::\d{2})?\b", text):
        return True
    if re.search(r"https?://|www\.|bit\.ly|\.com|\.net|\.io|êµ¬ë…|í´ë¦­|ë§í¬|ì´ë²¤íŠ¸|ì¶”ì²œì½”ë“œ|í• ì¸", text):
        return True
    return False

def extract_keywords_regex(text, top_n=3):
    words = re.findall(r"[ê°€-íž£a-zA-Z0-9]{2,}", str(text))
    if not words:
        return ""
    freq = Counter(words)
    return " | ".join([w for w, _ in freq.most_common(top_n)])

def softmax_np(x):
    e = np.exp(x - np.max(x))
    return e / e.sum(axis=-1, keepdims=True)

# ------------------------------
# ðŸ’¬ ê°ì • ì˜ˆì¸¡ê¸° ë¹Œë“œ
# ------------------------------
def build_sentiment_predictor(model_choice=None, neutral_threshold=0.6):
    tok, model, label_map = load_sentiment_model(model_choice)

    positive_words = ["ì¢‹", "ìµœê³ ", "ê°ë™", "ìž¬ë°Œ", "ë©‹", "ëŒ€ë°•", "êµ¿", "ì§±", "ðŸ‘", "ã…‹ã…‹", "ã…Žã…Ž", "ì˜ˆì˜", "ê·€ì—½", "ê°ì‚¬", "ê³ ë§™", "ì‚¬ëž‘", "ì™„ë²½", "í–‰ë³µ", "ã… ã… ", "ã…œã…œ"]
    negative_words = ["ì‹«", "ë³„ë¡œ", "ì´ìƒ", "ìµœì•…", "ë¬¸ì œ", "ë²„ê·¸", "ì‹¤ë§", "ì—†", "ì•„ë‹ˆ", "í™”ë‚¨", "ì§œì¦", "ì—ëŸ¬", "ë¶ˆíŽ¸", "ê°œíŒ"]

    def predict(text):
        if not isinstance(text, str) or len(text.strip()) < 2:
            return "ì¤‘ë¦½"
        inputs = tok(text, return_tensors="pt", truncation=True, padding=True).to(device)
        with torch.no_grad():
            logits = model(**inputs).logits.cpu().numpy()[0]
        probs = softmax_np(logits)
        pred = probs.argmax()
        conf = probs.max()
        label = label_map.get(pred, "ì¤‘ë¦½")

        if conf < neutral_threshold:
            if any(w in text for w in positive_words):
                label = "ê¸ì •"
            elif any(w in text for w in negative_words):
                label = "ë¶€ì •"
            else:
                label = "ì¤‘ë¦½"
        if any(w in text for w in positive_words):
            label = "ê¸ì •"
        elif any(w in text for w in negative_words):
            label = "ë¶€ì •"
        if ("ê°ì‚¬" in text or "ê³ ë§™" in text) and ("ã… " in text or "ã…œ" in text):
            label = "ê¸ì •"
        return label

    return predict

# ------------------------------
# ðŸ§¾ CSV ë¶„ì„ í•¨ìˆ˜
# ------------------------------
def analyze_keyword_csv(input_csv, output_csv):
    df = pd.read_csv(input_csv)
    if "video_title" not in df.columns:
        print("video_title ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    df["published_at"] = pd.to_datetime(df.get("published_at", datetime.now()), errors="coerce").fillna(datetime.now())
    df["date"] = df["published_at"].dt.date
    records = []
    for _, r in df.iterrows():
        words = re.findall(r"[ê°€-íž£a-zA-Z0-9]{2,}", str(r["video_title"]))
        for w, c in Counter(words).items():
            records.append({"keyword": w, "date": r["date"], "count": c})
    result = pd.DataFrame(records).groupby(["keyword", "date"])["count"].sum().reset_index()
    result.to_csv(output_csv, index=False, encoding="utf-8-sig")
    print(f"'{output_csv}' ìƒì„± ì™„ë£Œ ({len(result)}í–‰)")

def analyze_link_csv(input_csv, output_csv, args):
    df = pd.read_csv(input_csv)
    if "comment_text" not in df.columns:
        print("comment_text ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    if "comment_published" in df.columns:
        df["comment_published"] = pd.to_datetime(df["comment_published"], errors="coerce").fillna(datetime.now())
    else:
        df["comment_published"] = datetime.now()
    df.sort_values("comment_published", inplace=True)
    df = df[~df["comment_text"].apply(lambda x: is_spam_or_timeline(x, args.min_len))].reset_index(drop=True)
    print(f"í•„í„°ë§ í›„ ë‚¨ì€ ëŒ“ê¸€ ìˆ˜: {len(df)}ê°œ")

    extract_keywords = build_ner_extractor(args.use_ner)
    predict_sent = build_sentiment_predictor(args.sentiment_model, args.neutral_threshold)

    df["keyword"] = df["comment_text"].progress_apply(lambda x: extract_keywords(x))
    df["sentiment"] = df["comment_text"].progress_apply(predict_sent)

    result = df[["comment_text", "comment_published", "keyword", "sentiment"]].rename(
        columns={"comment_text": "comment", "comment_published": "time"}
    )
    result.to_csv(output_csv, index=False, encoding="utf-8-sig")
    print(f"'{output_csv}' ìƒì„± ì™„ë£Œ ({len(result)}í–‰)")

def auto_analyze_csv(input_csv, args):
    if not os.path.exists(input_csv):
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {input_csv}")
        return
    df = pd.read_csv(input_csv, nrows=5)
    if "comment_text" in df.columns:
        print("ë§í¬(ëŒ“ê¸€) CSVë¡œ íŒë³„ë¨")
        analyze_link_csv(input_csv, args.output or "analyzed_comments.csv", args)
    elif "video_title" in df.columns:
        print("í‚¤ì›Œë“œ(ì œëª©) CSVë¡œ íŒë³„ë¨")
        analyze_keyword_csv(input_csv, args.output or "analyzed_keywords.csv")
    else:
        print("CSV ìœ í˜•ì„ íŒë³„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ------------------------------
# ðŸš€ ì‹¤í–‰
# ------------------------------
def parse_cli_args():
    parser = argparse.ArgumentParser(description="YouTube CSV ìžë™ ë¶„ì„ê¸° (NER + ê°ì •ë¶„ì„)")
    parser.add_argument("--input", type=str, help="ë¶„ì„í•  CSV íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output", type=str, help="ê²°ê³¼ CSV ì €ìž¥ ê²½ë¡œ")
    parser.add_argument("--disable_ner", action="store_true", help="NER ë¹„í™œì„±í™” (ì •ê·œì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì‚¬ìš©)")
    parser.add_argument("--sentiment_model", choices=["kobert", "kcelectra"], default="kcelectra", help="ê°ì • ë¶„ì„ ëª¨ë¸ ì„ íƒ")
    parser.add_argument("--neutral_threshold", type=float, default=0.6, help="ì¤‘ë¦½ íŒë³„ ìž„ê³„ê°’")
    parser.add_argument("--min_len", type=int, default=3, help="ëŒ“ê¸€ ìµœì†Œ ê¸¸ì´ (ìŠ¤íŒ¸/íƒ€ìž„ë¼ì¸ í•„í„°)")
    parser.add_argument("--topn", type=int, default=3, help="í‚¤ì›Œë“œ ìƒìœ„ Nê°œ ì¶”ì¶œ")
    return parser.parse_args()


def resolve_path(base_dir, path):
    if not path:
        return None
    return path if os.path.isabs(path) else os.path.abspath(os.path.join(base_dir, path))


def main():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    cli_args = parse_cli_args()

    input_csv = resolve_path(base_dir, cli_args.input)
    if not input_csv:
        candidate_files = [
            os.path.join(base_dir, "youtube_link_results.csv"),
            os.path.join(base_dir, "youtube_keyword_results.csv"),
        ]
        input_csv = next((path for path in candidate_files if os.path.exists(path)), None)
        if not input_csv:
            print("ë¶„ì„í•  CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
    elif not os.path.exists(input_csv):
        print(f"íŒŒì¼ ì—†ìŒ: {input_csv}")
        return

    args = Args(
        output=resolve_path(base_dir, cli_args.output),
        use_ner=not cli_args.disable_ner,
        sentiment_model=cli_args.sentiment_model,
        neutral_threshold=cli_args.neutral_threshold,
        min_len=cli_args.min_len,
        topn=cli_args.topn,
    )

    auto_analyze_csv(input_csv, args)


if __name__ == "__main__":
    main()